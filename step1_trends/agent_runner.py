import sys
import os
import json

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from agents.trend_hunter import TrendHunterAgent

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
CONFIG_FILE = os.path.join(BASE_DIR, 'box_artist_config.json')
OUTPUT_FILE = os.path.join(BASE_DIR, 'generated_seo_data.json')

def run():
    print("\n" + "=" * 50)
    print("ğŸ¤– å¯åŠ¨ Agentic Workflow (Step 1: Trend Hunting)")
    print("=" * 50 + "\n")
    
    # Load Config
    if os.path.exists(CONFIG_FILE):
        with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
            args = json.load(f)
    else:
        args = {"mining_seeds": ["åŒ…è£…", "ç¤¼ç›’"]}
    hunter = TrendHunterAgent()
    topics = hunter.hunt_and_analyze(args)
    
    if topics:
        # æ ¹æ®é¡¹ç›®è¦æ±‚ï¼Œå°†æ‰€æœ‰æŒ–æ˜åˆ°çš„ä¸»é¢˜çŠ¶æ€ç»Ÿä¸€è®¾ç½®ä¸º "Ready"
        for t in topics:
            t['Status'] = 'Ready'
        # Save results
        # Save results (Append Mode)
        existing_data = []
        if os.path.exists(OUTPUT_FILE):
            with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
                try:
                    existing_data = json.load(f)
                except json.JSONDecodeError:
                    pass

        # Avoid duplicates (check by Topic name)
        existing_topics = {item['Topic'] for item in existing_data}
        new_count = 0
        for t in topics:
            if t['Topic'] not in existing_topics:
                existing_data.append(t)
                new_count += 1
        
        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
            json.dump(existing_data, f, ensure_ascii=False, indent=2)
            
        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜è‡³ {OUTPUT_FILE} (æ–°å¢ {new_count} æ¡, æ€»è®¡ {len(existing_data)} æ¡)")
        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜è‡³ {OUTPUT_FILE}")

        # Sync to Feishu (For Github Actions Persistence)
        if new_count > 0:
            print(f"â˜ï¸ æ­£åœ¨åŒæ­¥ {new_count} æ¡æ–°é€‰é¢˜åˆ°é£ä¹¦...")
            try:
                from shared.feishu_client import FeishuClient
                from shared import config
                
                client = FeishuClient()
                
                # Check duplicates in Feishu (This is expensive, so we just try batch create and ignore errors or rely on Feishu logic? 
                # Better: only upload what we determined as new locally)
                
                # Filter 'topics' to only include the ones we just added to existing_data
                # But 'topics' is the list of *newly generated* ones.
                # Among them, we filtered some out if they were in existing_data *before*.
                # Let's filter topics again to match the ones we appended.
                
                upload_list = []
                for t in topics:
                    # Check if this t was added. 
                    # We can re-use the logic: if t['Topic'] was not in existing_topics BEFORE update.
                    if t['Topic'] not in existing_topics:
                        record = {
                            "Topic": t['Topic'],
                            "å¤§é¡¹åˆ†ç±»": t['å¤§é¡¹åˆ†ç±»'],
                            "Status": config.STATUS_READY,
                            "é€‰é¢˜ç”Ÿæˆæ—¶é—´": t.get('created_at', '')
                        }
                        upload_list.append(record)
                
                if upload_list:
                    client.batch_create_records(upload_list)
                    print(f"âœ… å·²åŒæ­¥ {len(upload_list)} æ¡è®°å½•åˆ°é£ä¹¦")
                else:
                    print("âš ï¸ æ²¡æœ‰æ–°é€‰é¢˜éœ€è¦åŒæ­¥")
                    
            except Exception as e:
                print(f"âŒ é£ä¹¦åŒæ­¥å¤±è´¥: {e}")

if __name__ == "__main__":
    run()
