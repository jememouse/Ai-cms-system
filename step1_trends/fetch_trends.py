# trends_generator/fetch_trends.py
"""
çƒ­ç‚¹æŠ“å–ä¸ AI åˆ†ææ¨¡å—
"""
import requests
import json
import os
import re
import time
from datetime import datetime
from dotenv import load_dotenv

# åŠ è½½ .env ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½® DeepSeek API
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY", "sk-your-key-here")
DEEPSEEK_API_URL = "https://api.deepseek.com/v1/chat/completions"

# URLs
BAIDU_HOT_URL = "https://top.baidu.com/board?tab=realtime"
WEIBO_HOT_URL = "https://s.weibo.com/top/summary"
TOUTIAO_HOT_URL = "https://www.toutiao.com/hot-event/hot-board/?origin=toutiao_pc"
BILIBILI_HOT_URL = "https://api.bilibili.com/x/web-interface/ranking/v2?rid=0&type=all"
KR36_HOT_URL = "https://36kr.com/newsflashes"

# æ–‡ä»¶è·¯å¾„ (æŒ‡å‘é¡¹ç›®æ ¹ç›®å½•)
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
TRENDS_FILE = os.path.join(BASE_DIR, "trends_data.json")
CONFIG_FILE = os.path.join(BASE_DIR, "box_artist_config.json")
CACHE_FILE = os.path.join(BASE_DIR, ".cache", "trends_cache.json")
CACHE_EXPIRY_HOURS = 4  # ç¼“å­˜æœ‰æ•ˆæœŸ

def _load_cache():
    """åŠ è½½ç¼“å­˜æ•°æ®"""
    if not os.path.exists(CACHE_FILE):
        return {}
    try:
        with open(CACHE_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    except:
        return {}

def _save_cache(cache_data):
    """ä¿å­˜ç¼“å­˜æ•°æ®"""
    os.makedirs(os.path.dirname(CACHE_FILE), exist_ok=True)
    with open(CACHE_FILE, 'w', encoding='utf-8') as f:
        json.dump(cache_data, f, ensure_ascii=False, indent=2)

def _get_cached(key):
    """è·å–ç¼“å­˜ï¼Œæ£€æŸ¥æ˜¯å¦è¿‡æœŸ"""
    cache = _load_cache()
    if key in cache:
        cached_time = datetime.fromisoformat(cache[key].get("time", "2000-01-01"))
        if (datetime.now() - cached_time).total_seconds() < CACHE_EXPIRY_HOURS * 3600:
            print(f"   ğŸ“¦ ä½¿ç”¨ç¼“å­˜: {key}")
            return cache[key].get("data", [])
    return None

def _set_cached(key, data):
    """è®¾ç½®ç¼“å­˜"""
    cache = _load_cache()
    cache[key] = {"time": datetime.now().isoformat(), "data": data}
    _save_cache(cache)

# é€šç”¨ Header
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Cookie": "SUB=_2AkMSb-1af8NxqwJRmP0SzGvmZY1yyA_EieKkA3HJJRMxHRl-yT9kqmsstRB6POKqfE_JzXqqfE_JzXqqfE_JzXqq; _zap=a1b2c3d4; d_c0=abcd1234;" # ç®€å•çš„ Mock Cookie å¢åŠ æˆåŠŸç‡
}

def fetch_baidu_hot():
    """æŠ“å–ç™¾åº¦çƒ­æœæ¦œ"""
    print("ğŸ“¡ [Baidu] æ­£åœ¨æŠ“å–...")
    try:
        resp = requests.get(BAIDU_HOT_URL, headers=HEADERS, timeout=10)
        resp.encoding = 'utf-8'
        html = resp.text
        titles = re.findall(r'<div class="c-single-text-ellipsis">\s*(.*?)\s*</div>', html)
        clean_titles = [t.strip() for t in titles if t.strip() and "ç½®é¡¶" not in t][:15] # å–å‰15
        print(f"   -> è·å–åˆ° {len(clean_titles)} æ¡")
        return clean_titles
    except Exception as e:
        print(f"   âŒ å¤±è´¥: {e}")
        return []

def fetch_weibo_hot():
    """æŠ“å–å¾®åšçƒ­æœ"""
    print("ğŸ“¡ [Weibo] æ­£åœ¨æŠ“å–...")
    try:
        resp = requests.get(WEIBO_HOT_URL, headers=HEADERS, timeout=10)
        html = resp.text
        # å¾®åšæ ¼å¼: <a href="/weibo?q=xxx" target="_blank">xxx</a>
        # æ’é™¤ "javascript:void(0)" ç­‰ç½®é¡¶å¹¿å‘Š
        titles = re.findall(r'<a href="/weibo\?q=[^"]+" target="_blank">([^<]+)</a>', html)
        clean_titles = [t.strip() for t in titles if t.strip()][:15]
        print(f"   -> è·å–åˆ° {len(clean_titles)} æ¡")
        return clean_titles
    except Exception as e:
        print(f"   âŒ å¤±è´¥: {e}")
        return []

def fetch_toutiao_hot():
    """æŠ“å–å¤´æ¡çƒ­æ¦œ (æŠ–éŸ³/å­—èŠ‚ç³»æ•°æ®)"""
    print("ğŸ“¡ [Toutiao] æ­£åœ¨æŠ“å–...")
    try:
        resp = requests.get(TOUTIAO_HOT_URL, headers=HEADERS, timeout=10)
        data = resp.json()
        
        # è§£æå¤´æ¡ JSON ç»“æ„
        clean_titles = []
        if "fixed_top_data" in data:
            for item in data["fixed_top_data"]:
                clean_titles.append(item.get("Title"))
                
        if "data" in data:
            for item in data["data"]:
                clean_titles.append(item.get("Title"))
                
        # å–å‰15
        clean_titles = clean_titles[:15]
        print(f"   -> è·å–åˆ° {len(clean_titles)} æ¡")
        return clean_titles
    except Exception as e:
        print(f"   âŒ å¤±è´¥: {e}")
        return []

def fetch_bilibili_hot():
    """æŠ“å–Bç«™çƒ­é—¨è§†é¢‘ (å¹´è½»äººè¶‹åŠ¿)"""
    print("ğŸ“¡ [Bilibili] æ­£åœ¨æŠ“å–...")
    try:
        # Bç«™ API å¯¹ Cookie å¾ˆæ•æ„Ÿï¼Œæœ‰æ—¶ç”šè‡³ä¸éœ€è¦ Cookie åªè¦ UA
        # è¿™é‡Œå•ç‹¬å®šä¹‰ Headerï¼Œä¸å¸¦ Cookie
        headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }
        resp = requests.get(BILIBILI_HOT_URL, headers=headers, timeout=10)
        data = resp.json()
        
        clean_titles = []
        if data.get("code") == 0 and "data" in data and "list" in data["data"]:
            for item in data["data"]["list"]:
                clean_titles.append(item.get("title"))
                
        clean_titles = clean_titles[:15]
        print(f"   -> è·å–åˆ° {len(clean_titles)} æ¡")
        return clean_titles
    except Exception as e:
        print(f"   âŒ å¤±è´¥: {e}")
        return []

def fetch_36kr_hot():
    """æŠ“å–36æ°ªå¿«è®¯ (è¡Œä¸š/å•†ä¸š)"""
    print("ğŸ“¡ [36Kr] æ­£åœ¨æŠ“å–...")
    try:
        resp = requests.get(KR36_HOT_URL, headers=HEADERS, timeout=10)
        html = resp.text
        
        # ä¼˜åŒ–æå–é€»è¾‘ï¼Œä¸ç”¨ç®€å•çš„æ­£åˆ™é˜²æ­¢æå‰æˆªæ–­
        start_marker = "window.initialState="
        if start_marker in html:
            start_idx = html.find(start_marker) + len(start_marker)
            # æ‰¾åˆ°éšåçš„è„šæœ¬ç»“æŸæ ‡ç­¾æˆ–è€…åˆ†å·
            # ä½† JSON å¯èƒ½åŒ…å«åˆ†å·ï¼Œæœ€ç¨³çš„æ˜¯æ‰¾ </script>
            end_idx = html.find("</script>", start_idx)
            
            json_str = html[start_idx:end_idx].strip()
            # å»æ‰æœ«å°¾å¯èƒ½çš„åˆ†å·
            if json_str.endswith(";"):
                json_str = json_str[:-1]
                
            try:
                data = json.loads(json_str)
                clean_titles = []
                # è·¯å¾„: newsflashCatalogData -> data -> newsflashList -> data -> itemList
                items = data.get("newsflashCatalogData", {}).get("data", {}).get("newsflashList", {}).get("data", {}).get("itemList", [])
                for item in items:
                    title = item.get("templateMaterial", {}).get("widgetTitle")
                    if title:
                        clean_titles.append(title)
                
                clean_titles = clean_titles[:15]
                print(f"   -> è·å–åˆ° {len(clean_titles)} æ¡")
                return clean_titles
                
            except json.JSONDecodeError:
                print("   âš ï¸ 36Kr: JSON è§£æå¤±è´¥")
                return []
        else:
            print("   âš ï¸ 36Kr: æœªæ‰¾åˆ°æ•°æ®æºæ ‡è®°")
            return []
            
    except Exception as e:
        print(f"   âŒ å¤±è´¥: {e}")
        return []

def fetch_zhihu_hot_questions(seed_words):
    """æŠ“å–çŸ¥ä¹çƒ­é—¨é—®ç­” (é«˜æ„å›¾é—®ç­”ï¼Œé€‚åˆ GEO ä¼˜åŒ–)"""
    if not seed_words:
        return []
        
    print(f"â“ å¼€å§‹æŒ–æ˜çŸ¥ä¹é—®ç­”ï¼ˆé«˜æ„å›¾éœ€æ±‚ï¼‰...")
    questions = []
    import random
    # éšæœºé€‰å– 8 ä¸ªç§å­è¯è¿›è¡ŒæŒ–æ˜
    target_seeds = random.sample(seed_words, min(8, len(seed_words)))
    
    for seed in target_seeds:
        try:
            # çŸ¥ä¹æœç´¢ API (ç®€åŒ–ç‰ˆï¼Œé€šè¿‡ç½‘é¡µæ¥å£)
            url = f"https://www.zhihu.com/api/v4/search_v3?t=general&q={seed}&offset=0&limit=5"
            headers = {
                "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36",
                "Referer": "https://www.zhihu.com/search"
            }
            resp = requests.get(url, headers=headers, timeout=8)
            if resp.status_code == 200:
                data = resp.json()
                if "data" in data:
                    for item in data["data"][:3]:  # æ¯ä¸ªç§å­è¯å–å‰3æ¡
                        obj = item.get("object", {})
                        # ä¼˜å…ˆè·å–é—®é¢˜æ ‡é¢˜
                        if item.get("type") == "search_result":
                            title = obj.get("title", "") or obj.get("question", {}).get("title", "")
                            if title and len(title) > 5:
                                # æ¸…ç† HTML æ ‡ç­¾
                                clean_title = re.sub(r'<[^>]+>', '', title)
                                questions.append(f"[çŸ¥ä¹é—®ç­”] {clean_title}")
                    print(f"   -> '{seed}' æŒ–åˆ°: {min(3, len(data.get('data', [])))} æ¡")
            
            time.sleep(0.8)  # çŸ¥ä¹åçˆ¬ä¸¥æ ¼ï¼Œå¢åŠ é—´éš”
        except Exception as e:
            print(f"   âš ï¸ çŸ¥ä¹æŒ–æ˜ '{seed}' å¤±è´¥: {e}")
            
    # å»é‡
    questions = list(set(questions))
    print(f"   -> æ€»è®¡è·å– {len(questions)} ä¸ªçŸ¥ä¹é«˜æ„å›¾é—®ç­”")
    return questions

def fetch_xiaohongshu_trends(seed_words):
    """æŠ“å–å°çº¢ä¹¦çƒ­é—¨è¯é¢˜ (Cç«¯æ¶ˆè´¹è¶‹åŠ¿ï¼Œå¹´è½»ç¾¤ä½“åå¥½)"""
    if not seed_words:
        return []
        
    print(f"ğŸ“• å¼€å§‹æŒ–æ˜å°çº¢ä¹¦æ¶ˆè´¹è¶‹åŠ¿...")
    trends = []
    import random
    # éšæœºé€‰å– 6 ä¸ªç§å­è¯
    target_seeds = random.sample(seed_words, min(6, len(seed_words)))
    
    for seed in target_seeds:
        try:
            # å°çº¢ä¹¦æœç´¢å»ºè®® API (å…¬å¼€æ¥å£)
            url = f"https://edith.xiaohongshu.com/api/sns/web/v1/search/hot_list"
            headers = {
                "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36",
                "Referer": "https://www.xiaohongshu.com/"
            }
            resp = requests.get(url, headers=headers, timeout=8)
            
            if resp.status_code == 200:
                data = resp.json()
                if data.get("success") and "data" in data:
                    hot_list = data["data"].get("list", [])[:10]
                    for item in hot_list:
                        title = item.get("title", "")
                        if title and any(kw in title for kw in ["åŒ…è£…", "ç¤¼ç›’", "é€ç¤¼", "å¼€ç®±", "å¥½ç‰©"]):
                            trends.append(f"[å°çº¢ä¹¦] {title}")
                    print(f"   -> è·å–åˆ° {len(hot_list)} ä¸ªçƒ­é—¨è¯é¢˜")
                    break  # çƒ­æ¦œåªéœ€è¯·æ±‚ä¸€æ¬¡
            
            time.sleep(0.5)
        except Exception as e:
            print(f"   âš ï¸ å°çº¢ä¹¦æŠ“å–å¤±è´¥: {e}")
    
    # å¤‡ç”¨ï¼šåŸºäºç§å­è¯æ„é€ æ¶ˆè´¹åœºæ™¯è¯é¢˜
    consumption_scenes = [
        "å¼€ç®±ä½“éªŒ", "é€ç¤¼æ¨è", "é«˜çº§æ„ŸåŒ…è£…", "æ‹†å¿«é€’", 
        "å¥½ç‰©åˆ†äº«", "é¢œå€¼åŒ…è£…", "ç²¾è‡´ç”Ÿæ´»"
    ]
    for seed in target_seeds[:3]:
        for scene in random.sample(consumption_scenes, 2):
            trends.append(f"[å°çº¢ä¹¦] {seed}{scene}")
    
    trends = list(set(trends))
    print(f"   -> æ€»è®¡è·å– {len(trends)} ä¸ªå°çº¢ä¹¦è¶‹åŠ¿")
    return trends

def fetch_google_trends(seed_words):
    """è·å–è°·æ­Œè¶‹åŠ¿æ•°æ® (æµ·å¤–å¸‚åœºæ´å¯Ÿï¼Œè·¨å¢ƒç”µå•†éœ€æ±‚)"""
    if not seed_words:
        return []
        
    print(f"ğŸŒ å¼€å§‹è·å–è°·æ­Œå…¨çƒè¶‹åŠ¿...")
    trends = []
    
    # åŒ…è£…è¡Œä¸šæµ·å¤–å…³é”®è¯
    overseas_keywords = [
        "custom packaging", "gift box wholesale", "mailer box",
        "packaging design trends", "sustainable packaging",
        "luxury packaging", "eco friendly packaging",
        "packaging supplier", "corrugated box manufacturer"
    ]
    
    for kw in overseas_keywords[:5]:
        try:
            # Google Trends å»ºè®® API (ç®€åŒ–ç‰ˆ)
            url = f"https://trends.google.com/trends/api/autocomplete/{kw.replace(' ', '%20')}?hl=en-US"
            headers = {
                "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
            }
            resp = requests.get(url, headers=headers, timeout=8)
            
            if resp.status_code == 200:
                # Google Trends è¿”å›éœ€è¦å¤„ç†å‰ç¼€
                text = resp.text
                if text.startswith(")]}'"):
                    text = text[5:]
                try:
                    data = json.loads(text)
                    if "default" in data and "topics" in data["default"]:
                        for topic in data["default"]["topics"][:3]:
                            title = topic.get("title", "")
                            if title:
                                trends.append(f"[è°·æ­Œè¶‹åŠ¿] {title}")
                except:
                    pass
            
            time.sleep(0.3)
        except Exception as e:
            print(f"   âš ï¸ è°·æ­Œè¶‹åŠ¿ '{kw}' è·å–å¤±è´¥: {e}")
    
    # å¤‡ç”¨ï¼šé¢„è®¾æµ·å¤–çƒ­é—¨è¯é¢˜
    preset_trends = [
        "[è°·æ­Œè¶‹åŠ¿] sustainable packaging solutions 2026",
        "[è°·æ­Œè¶‹åŠ¿] custom mailer boxes for small business",
        "[è°·æ­Œè¶‹åŠ¿] eco friendly packaging alternatives",
        "[è°·æ­Œè¶‹åŠ¿] luxury gift box packaging design",
        "[è°·æ­Œè¶‹åŠ¿] corrugated shipping boxes wholesale"
    ]
    trends.extend(preset_trends)
    
    trends = list(set(trends))
    print(f"   -> æ€»è®¡è·å– {len(trends)} ä¸ªæµ·å¤–è¶‹åŠ¿")
    return trends

def fetch_baidu_suggestions(seed_words):
    """æŒ–æ˜ç™¾åº¦ä¸‹æ‹‰æ¨èè¯ (ç²¾å‡†æœç´¢éœ€æ±‚)"""
    if not seed_words:
        return []
        
    print(f"â›ï¸  å¼€å§‹æŒ–æ˜ {len(seed_words)} ä¸ªç§å­è¯çš„é•¿å°¾éœ€æ±‚...")
    suggestions = []
    
    for seed in seed_words:
        try:
            # Baidu Suggest API: window.bdsug.sug({q:"...",s:["..."]})
            url = f"http://suggestion.baidu.com/su?wd={seed}&p=3&cb=window.bdsug.sug"
            resp = requests.get(url, headers=HEADERS, timeout=5)
            # ç®€å•çš„æ­£åˆ™æå– s:[...] å†…å®¹
            match = re.search(r's:(\[.*?\])', resp.text)
            if match:
                # è½¬æ¢ç±»ä¼¼ JSON çš„æ•°ç»„å­—ç¬¦ä¸² (è™½ç„¶å®ƒæ˜¯ JS æ•°ç»„ï¼Œä½† python json ä¹Ÿèƒ½è§£å¤§éƒ¨åˆ†)
                # æ³¨æ„ï¼šBaidu æœ‰æ—¶è¿”å›å•å¼•å·ï¼Œpy json éœ€è¦åŒå¼•å·
                raw_list = match.group(1).replace("'", '"')
                try:
                    words = json.loads(raw_list)
                    # é€‰å–å‰ 5 ä¸ªæœ€ç›¸å…³çš„
                    top_words = words[:5]
                    for w in top_words:
                        suggestions.append(f"[æœç´¢éœ€æ±‚] {w}")
                    print(f"   -> '{seed}' æŒ–åˆ°: {len(top_words)} ä¸ª")
                except:
                    pass
            time.sleep(0.5) 
        except Exception as e:
            print(f"   âŒ æŒ–æ˜ '{seed}' å¤±è´¥: {e}")
            
    print(f"   -> æ€»è®¡è·å– {len(suggestions)} ä¸ªç™¾åº¦é•¿å°¾éœ€æ±‚")
    return suggestions

def fetch_1688_suggestions(seed_words):
    """æŒ–æ˜1688ä¸‹æ‹‰æ¨èè¯ (B2Bæºå¤´é‡‡è´­éœ€æ±‚)"""
    if not seed_words:
        return []
        
    print(f"ğŸ­ å¼€å§‹æŒ–æ˜ 1688 (B2B) é•¿å°¾éœ€æ±‚...")
    suggestions = []
    import random
    # éšæœºé€‰å– 10 ä¸ªè¯è¿›è¡ŒæŒ–æ˜ï¼Œé˜²æ­¢è¯·æ±‚è¿‡å¤š
    target_seeds = random.sample(seed_words, min(10, len(seed_words)))
    
    for seed in target_seeds:
        try:
            # 1688 Suggest API
            url = f"https://suggest.1688.com/bin/suggest?code=utf-8&q={seed}"
            resp = requests.get(url, headers=HEADERS, timeout=5)
            data = resp.json()
            if "result" in data:
                top_words = [item['q'] for item in data['result'][:5]]
                for w in top_words:
                    suggestions.append(f"[1688é‡‡è´­] {w}")
                print(f"   -> '{seed}' æŒ–åˆ°: {len(top_words)} ä¸ª")
            time.sleep(0.5)
        except Exception as e:
            print(f"   âŒ 1688æŒ–æ˜ '{seed}' å¤±è´¥: {e}")
            
    print(f"   -> æ€»è®¡è·å– {len(suggestions)} ä¸ª1688é•¿å°¾éœ€æ±‚")
    return suggestions

def fetch_taobao_suggestions(seed_words):
    """æŒ–æ˜æ·˜å®ä¸‹æ‹‰æ¨èè¯ (Cç«¯æ¶ˆè´¹è¶‹åŠ¿)"""
    if not seed_words:
        return []

    print(f"ğŸ›ï¸  å¼€å§‹æŒ–æ˜æ·˜å® (Cç«¯) æ¶ˆè´¹è¶‹åŠ¿...")
    suggestions = []
    import random
    target_seeds = random.sample(seed_words, min(10, len(seed_words)))

    for seed in target_seeds:
        try:
            # Taobao Suggest API
            url = f"https://suggest.taobao.com/sug?code=utf-8&q={seed}&k=1&area=c2c"
            resp = requests.get(url, headers=HEADERS, timeout=5)
            data = resp.json()
            if "result" in data:
                top_words = [item[0] for item in data['result'][:5]]
                for w in top_words:
                    suggestions.append(f"[æ·˜å®çƒ­æœ] {w}")
                print(f"   -> '{seed}' æŒ–åˆ°: {len(top_words)} ä¸ª")
            time.sleep(0.5)
        except Exception as e:
            print(f"   âŒ æ·˜å®æŒ–æ˜ '{seed}' å¤±è´¥: {e}")

    print(f"   -> æ€»è®¡è·å– {len(suggestions)} ä¸ªæ·˜å®é•¿å°¾éœ€æ±‚")
    return suggestions

def analyze_trends_with_ai(trends):
    """ä½¿ç”¨ DeepSeek åˆ†æçƒ­æœä¸åŒ…è£…è¡Œä¸šçš„å…³è”"""
    if not trends:
        return []
        
    print(f"ğŸ§  æ­£åœ¨è¯·æ±‚ DeepSeek åˆ†æ {len(trends)} ä¸ªè¯é¢˜...")
    
    # æ„é€  Prompt
    trends_str = "\n".join([f"- {t}" for t in trends])
    prompt = f"""
    æˆ‘æ˜¯ä¸€ä¸ªåšã€åŒ…è£…å°åˆ·ã€ç¤¼ç›’å®šåˆ¶ã€å“ç‰Œè®¾è®¡ã€‘çš„å·¥å‚ã€‚
    è¯·åˆ†æä»¥ä¸‹å…¨ç½‘çƒ­ç‚¹ï¼Œ**åŠ¡å¿…æŒ‘é€‰å‡º 25 ä¸ª** æœ€é€‚åˆå†™æ–‡ç« çš„è¯é¢˜ï¼ˆæ•°é‡ä¸è¶³æ‰£åˆ†ï¼‰ã€‚
    
    **ç­›é€‰ä¼˜å…ˆçº§ï¼ˆGEO æ—¶ä»£ç²¾å‡†è¥é”€ç‰ˆ 2026ï¼‰ï¼š**
    1. **Sçº§ï¼ˆå¿…é€‰ - é«˜æ„å›¾éœ€æ±‚ï¼‰**ï¼š
       - å¸¦æœ‰ `[æœç´¢éœ€æ±‚]` æˆ– `[1688é‡‡è´­]` æ ‡è®°çš„å†…å®¹ï¼ˆç”¨æˆ·å·²æœ‰æ˜ç¡®é‡‡è´­æ„å‘ï¼‰
       - **é—®ç­”ç±»è¯é¢˜**ï¼šå¦‚"XXæ€ä¹ˆé€‰"ã€"XXå¤šå°‘é’±"ã€"XXå“ªå®¶å¥½"ï¼ˆé€‚åˆ AI æœç´¢å¼•æ“æ‘˜å½•ï¼‰
       - **æŠ€æœ¯ç±»é•¿å°¾éœ€æ±‚**ï¼šå°åˆ·è®¾å¤‡ä»‹ç»/ç»´ä¿®ã€è¡Œä¸šæ ‡å‡†è§£è¯»ã€åŒ…è£…è®¡ç®—å…¬å¼
       - **è¡Œä¸šå‰æ²¿è¶‹åŠ¿**ï¼šæ•°å­—åŒ–ã€AIã€å‡ºæµ·ã€å¯æŒç»­åŒ…è£…
    2. **Açº§ï¼ˆé‡ç‚¹ - å•†ä¸šå…³è”ï¼‰**ï¼š
       - èƒ½å…³è”åˆ°"å®ä½“äº§å“ã€ç¤¼å“ç»æµã€æ¶ˆè´¹è¡Œä¸šï¼ˆç¾å¦†/é£Ÿå“/ç”µå­ï¼‰"çš„å•†ä¸šçƒ­ç‚¹
       - å¸¦æœ‰æ˜ç¡®åœºæ™¯çš„è¯é¢˜ï¼ˆå¦‚ï¼š"æ˜¥èŠ‚ç¤¼ç›’"ã€"ç”µå•†åŒ…è£…"ã€"å¤–å–åŒ…è£…"ï¼‰
       - **è¡Œä¸šæ´»åŠ¨ï¼ˆæ–°å¢é‡ç‚¹ï¼‰**ï¼šåŒ…è£…å±•ä¼šã€è®¾è®¡å¤§èµ›ã€é«˜å³°è®ºå›ã€æŠ€æœ¯äº¤æµä¼šï¼ˆå¿…é¡»å½’ç±»ä¸º'è¡Œä¸šèµ„è®¯'ï¼‰
       - **ä¸‹æ¸¸è¡Œä¸šå±•ä¼šï¼ˆSçº§å•†æœºï¼‰**ï¼šé£Ÿå“å±•ã€ç¾åšä¼šã€ç”µå­å±•ã€ç¤¼å“å±•ï¼ˆéœ€åˆ†æå…¶å¯¹åŒ…è£…çš„æ–°éœ€æ±‚ï¼‰
    3. **Bçº§ï¼ˆç‰¹å®šå…³è”ï¼‰**ï¼š
       - èƒ½å¼ºè¡Œå…³è”è¡Œä¸šæ ‡å‡†çš„ç¤¾ä¼šçƒ­ç‚¹ï¼ˆå¦‚ï¼š"ç¯ä¿æ”¿ç­–â†’ç»¿è‰²åŒ…è£…"ã€"å¿«é€’æ–°è§„â†’æŠ—å‹çº¸ç®±"ï¼‰
    4. **Dçº§ï¼ˆåšå†³å‰”é™¤ï¼‰**ï¼š
       - çº¯å¨±ä¹å…«å¦ã€æ”¿æ²»æ•æ„Ÿã€è´Ÿé¢ç¤¾ä¼šæ–°é—»
       - æ— æ³•æä¾›"å®ç”¨ä»·å€¼"çš„è¯é¢˜ï¼ˆAI æœç´¢å¼•æ“ä¸ä¼šæ¨èæ— ä»·å€¼å†…å®¹ï¼‰

    **GEO æ—¶ä»£è¥é”€æ€è€ƒï¼ˆæ–°å¢ï¼‰ï¼š**
    - çœ‹åˆ°"XXæ€ä¹ˆé€‰"ï¼Œæ€è€ƒï¼šè¿™æ˜¯é«˜æ„å›¾é—®ç­”ï¼ŒAI ä¼šä¼˜å…ˆæ¨èæœ‰æ¸…æ™°ç­”æ¡ˆçš„æ–‡ç«  âœ“
    - çœ‹åˆ°"XXå¤šå°‘é’±"ï¼Œæ€è€ƒï¼šç”¨æˆ·æœ‰é‡‡è´­æ„å‘ï¼Œå¯ä»¥å†™ä»·æ ¼ç§‘æ™®+æŠ¥ä»·å¼•å¯¼ âœ“
    - çœ‹åˆ°"XXå±•ä¼š/è®ºå›/å¤§èµ›"ï¼Œæ€è€ƒï¼šè¿™æ˜¯è¡Œä¸šèµ„è®¯çš„é«˜ä»·å€¼å†…å®¹ï¼Œ**ä¼˜å…ˆä¿ç•™**ï¼Œç”¨æˆ·å…³æ³¨è¡Œä¸šåŠ¨æ€ âœ“
    - çœ‹åˆ°çº¯çƒ­ç‚¹äº‹ä»¶ï¼Œæ€è€ƒï¼šèƒ½å¦è½¬åŒ–ä¸º"å®ç”¨æ•™ç¨‹"æˆ–"é¿å‘æŒ‡å—"ï¼Ÿèƒ½â†’é€‰ï¼Œå¦â†’å¼ƒ

    çƒ­æœåˆ—è¡¨ï¼ˆå·²æ ‡è®°æ¥æºï¼‰ï¼š
    {trends_str}
    
    å¯¹äºæ¯ä¸ªæŒ‘é€‰å‡ºçš„ç›¸å…³è¯é¢˜ï¼Œè¯·ç»™å‡ºï¼ˆè¯·ä¿ç•™åŸå§‹è¯é¢˜ä¸­çš„[æ¥æº]æ ‡è®°ï¼‰ï¼š
    1. topic: è¯é¢˜åç§° (e.g. "[æœç´¢éœ€æ±‚] åŒ…è£…å®šåˆ¶å“ªå®¶å¥½")
    2. angle: ç»“åˆè§’åº¦ (ä¾‹å¦‚ï¼šåˆ†æäº‹ä»¶ä¸­çš„ç¤¼å“åŒ…è£…å·®å¼‚ã€çƒ­ç‚¹äººç‰©å¸¦ç«çš„åŒæ¬¾è‰²ç³»ç­‰)
    3. content_type: å»ºè®®çš„å†…å®¹å½¢å¼ï¼Œå¯é€‰å€¼ï¼š
       - "é—®ç­”ç§‘æ™®"ï¼šé€‚åˆ"XXæ˜¯ä»€ä¹ˆ"ç±»è¯é¢˜
       - "å¯¹æ¯”è¯„æµ‹"ï¼šé€‚åˆ"XX vs XX"ã€"å“ªä¸ªå¥½"ç±»è¯é¢˜
       - "æ•™ç¨‹æŒ‡å—"ï¼šé€‚åˆ"æ€ä¹ˆåš"ã€"å¦‚ä½•"ç±»è¯é¢˜
       - "ä»·æ ¼æ­ç§˜"ï¼šé€‚åˆ"å¤šå°‘é’±"ã€"ä»·æ ¼"ç±»è¯é¢˜
       - "è¶‹åŠ¿åˆ†æ"ï¼šé€‚åˆè¡Œä¸šåŠ¨æ€ç±»è¯é¢˜
    4. priority: ä¼˜å…ˆçº§ (S/A/B)
    
    è¯·ä¸¥æ ¼è¿”å› JSON æ ¼å¼åˆ—è¡¨ï¼š
    [
        {{"topic": "è¯é¢˜å", "angle": "ç»“åˆè§’åº¦", "content_type": "é—®ç­”ç§‘æ™®", "priority": "S"}}
    ]
    ä¸è¦è¿”å› Markdownã€‚
    """
    
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {DEEPSEEK_API_KEY}"
    }
    
    payload = {
        "model": "deepseek-chat",
        "messages": [
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.7
    }
    
    try:
        if "sk-your-key-here" in DEEPSEEK_API_KEY:
            print("âš ï¸ æœªé…ç½® DeepSeek Keyï¼Œè·³è¿‡ AI åˆ†æã€‚")
            return []

        # æ•°æ®é‡å¤§æ—¶ï¼ŒAPI å“åº”è¾ƒæ…¢ï¼Œå¢åŠ è¶…æ—¶æ—¶é—´åˆ° 120ç§’
        resp = requests.post(DEEPSEEK_API_URL, headers=headers, json=payload, timeout=120)
        result = resp.json()
        
        if "choices" in result:
            content = result["choices"][0]["message"]["content"]
            content = content.replace("```json", "").replace("```", "").strip()
            return json.loads(content)
        else:
            print(f"âŒ API è°ƒç”¨é”™è¯¯: {result}")
            return []
            
    except Exception as e:
        print(f"âŒ AI åˆ†æå¤±è´¥: {e}")
        return []

def main():
    # 0. è¯»å–é…ç½®è·å–ç§å­è¯
    mining_seeds = []
    if os.path.exists(CONFIG_FILE):
        with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
            cfg = json.load(f)
            mining_seeds = cfg.get("mining_seeds", [])

    # 1. å¤šæºæŠ“å–
    all_trends = []
    
    # æŒ–æ˜é•¿å°¾éœ€æ±‚ (ä¼˜å…ˆ)
    all_trends.extend(fetch_baidu_suggestions(mining_seeds))
    all_trends.extend(fetch_1688_suggestions(mining_seeds))
    all_trends.extend(fetch_taobao_suggestions(mining_seeds))
    all_trends.extend(fetch_zhihu_hot_questions(mining_seeds))  # çŸ¥ä¹é«˜æ„å›¾é—®ç­”
    all_trends.extend(fetch_xiaohongshu_trends(mining_seeds))   # å°çº¢ä¹¦æ¶ˆè´¹è¶‹åŠ¿
    all_trends.extend(fetch_google_trends(mining_seeds))        # è°·æ­Œæµ·å¤–è¶‹åŠ¿
    
    # æ‰‹åŠ¨æ ‡è®°æ¥æº
    for t in fetch_baidu_hot():
        all_trends.append(f"[ç™¾åº¦] {t}")
        
    for t in fetch_weibo_hot():
        all_trends.append(f"[å¾®åš] {t}")
        
    for t in fetch_toutiao_hot():
        all_trends.append(f"[å¤´æ¡] {t}")
        
    # Bç«™åçˆ¬ä¸¥é‡æš‚è·³è¿‡
    # for t in fetch_bilibili_hot():
    #     all_trends.append(f"[Bç«™] {t}")
        
    for t in fetch_36kr_hot():
        all_trends.append(f"[36æ°ª] {t}")
    
    # å»é‡
    unique_trends = list(set(all_trends))
    print(f"ğŸ“Š å…±æ”¶é›†åˆ° {len(unique_trends)} ä¸ªå”¯ä¸€çƒ­ç‚¹è¯é¢˜")

    # 2. åˆ†æ
    analyzed_data = analyze_trends_with_ai(unique_trends)
    
    # 3. å­˜å‚¨
    output = {
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "raw_trends_count": len(unique_trends),
        "all_trends_list": unique_trends,  # ä¿å­˜æ‰€æœ‰åŸå§‹çƒ­ç‚¹
        "analyzed_trends": analyzed_data
    }
    
    with open(TRENDS_FILE, 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
        
    print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜è‡³ {TRENDS_FILE}")

if __name__ == "__main__":
    main()