import sys
import os
import random
import logging
from datetime import datetime
from typing import List, Dict

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from core.skill import BaseSkill
from shared import llm_utils

# é…ç½® logger
logger = logging.getLogger(__name__)

class TopicAnalysisSkill(BaseSkill):
    """
    æŠ€èƒ½: è¯é¢˜åˆ†æå¸ˆ (ä½¿ç”¨ LLM åˆ†æçƒ­ç‚¹å¹¶ç”Ÿæˆé€‰é¢˜)
    """
    def __init__(self):
        super().__init__(
            name="topic_analysis",
            description="åˆ†æçƒ­ç‚¹åˆ—è¡¨ï¼ŒæŒ‘é€‰æœ€æœ‰ä»·å€¼çš„ 20 ä¸ªï¼Œå¹¶ç”Ÿæˆ 6 ä¸ª SEO æ ‡é¢˜"
        )

    def execute(self, input_data: Dict) -> List[Dict]:
        """
        Input: {"trends": [], "config": {}}
        Output: [{"Topic": "...", "å¤§é¡¹åˆ†ç±»": "...", ...}]
        """
        trends = input_data.get("trends", [])
        if not trends: return []

        # 1. ç¬¬ä¸€æ­¥ï¼šç­›é€‰ 20 ä¸ªçƒ­ç‚¹
        analyzed_trends = self._analyze_trends(trends)
        
        results = []
        generated_texts = [] # ç”¨äºå»é‡æ£€æŸ¥

        # 2. ç¬¬äºŒæ­¥ï¼šä¸ºæ¯ä¸ªçƒ­ç‚¹ç”Ÿæˆæ ‡é¢˜
        for idx, trend in enumerate(analyzed_trends):
            print(f"   ğŸ§  [Analyst] ç”Ÿæˆæ ‡é¢˜ ({idx+1}/{len(analyzed_trends)}): {trend['topic']}")
            titles = self._generate_titles(trend, input_data.get("config", {}))
            
            for t in titles:
                raw_title = t['title'].strip()
                
                # [Deduplication] æŸ¥é‡
                if self._is_text_similar(raw_title, generated_texts):
                    print(f"   ğŸ—‘ï¸ [Dedupe] ä¸¢å¼ƒé«˜ç›¸ä¼¼åº¦æ ‡é¢˜: {raw_title}")
                    continue
                
                generated_texts.append(raw_title)
                
                results.append({
                    "Topic": raw_title,
                    "å¤§é¡¹åˆ†ç±»": self._clean_category(t['category']),
                    "Status": "Pending",
                    "Source_Trend": trend['topic'],
                    "created_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                })
        
        return results

    def _is_text_similar(self, new_text: str, existing_texts: List[str], threshold: float = 0.6) -> bool:
        """
        ç®€å•çš„æ–‡æœ¬ç›¸ä¼¼åº¦å»é‡ (Jaccard Similarity on chars)
        """
        if not existing_texts: return False
        
        s1 = set(new_text)
        for t in existing_texts:
            s2 = set(t)
            intersection = len(s1.intersection(s2))
            union = len(s1.union(s2))
            if union == 0: continue
            
            sim = intersection / union
            # å¦‚æœå‰ 5 ä¸ªå­—å®Œå…¨ä¸€æ ·ï¼Œä¹Ÿè§†ä¸ºé‡å¤
            if new_text[:5] == t[:5]:
                return True
                
            if sim > threshold:
                return True
        return False

    def _analyze_trends(self, trends):
        import re
        trends_str = "\n".join([f"- {t}" for t in trends])

        # åŠ¨æ€é€‰æ‹©åŸå¸‚ (GEO ç­–ç•¥)
        GEO_CITIES = ["ä¸œè", "æ·±åœ³", "å¹¿å·", "ä¸Šæµ·", "æ­å·", "è‹å·", "ä¹‰ä¹Œ", "ä½›å±±"]
        selected_city = random.choice(GEO_CITIES)
        
        trend_settings = input_data.get("config", {}).get("trend_settings", {})
        target_count = trend_settings.get("max_trends_to_analyze", 5)

        prompt = f"""
        æˆ‘ä»¬æ˜¯ä¸€å®¶ **"åŒ…è£…åœ¨çº¿å®šåˆ¶ç”µå•†å¹³å°ï¼ˆå¼ºå¤§çš„ä¾›åº”é“¾åŠå“è´¨ç®¡ç†+äº¤ä»˜ç³»ç»Ÿï¼‰"** ï¼ˆç›’è‰ºå®¶ï¼‰ã€‚
        ä½ æ˜¯ä¸€ä½æ‹¥æœ‰10å¹´ç»éªŒçš„åŒ…è£…è§£å†³æ–¹æ¡ˆä¸“å®¶ï¼Œä»£è¡¨ **ç›’è‰ºå®¶ï¼ˆåŒ…è£…åœ¨çº¿å®šåˆ¶å¹³å° + å¼ºå¤§çš„ä¾›åº”é“¾åŠå“è´¨ç®¡ç†+äº¤ä»˜ç³»ç»Ÿï¼‰**ã€‚æ“…é•¿åŒæ—¶æœåŠ¡ **B2Bä¼ä¸šé‡‡è´­** å’Œ **B2C/C2Mä¸ªäººå®šåˆ¶**ã€‚å³ä½¿æ˜¯é€šç”¨è¯é¢˜ï¼Œä¹Ÿè¦åŸºäº **{selected_city}** çš„åœ°åŸŸè§†è§’è¿›è¡Œè§£ç­”ã€‚

        è¯·ä»ä»¥ä¸‹å…¨ç½‘çƒ­ç‚¹ä¸­ï¼Œ**åŠ¡å¿…æŒ‘é€‰å‡º {target_count} ä¸ª** æœ€é€‚åˆå†™æ–‡ç« çš„è¯é¢˜ã€‚

        ç­›é€‰ä¼˜å…ˆçº§ï¼ˆå…¼é¡¾ B2B ä¸ B2Cï¼‰ï¼š
        1. **Sçº§ï¼ˆå¿…é€‰ - å€ŸåŠ¿è¥é”€/é«˜æ„å›¾ï¼‰**ï¼š
           - **ç¤¾ä¼šçƒ­ç‚¹å¼ºå…³è” (Newsjacking)**ï¼šèƒ½é€šè¿‡"éšå–»/åœºæ™¯/é…è‰²"å¼ºè¡Œå…³è”çš„ç ´åœˆçƒ­ç‚¹ã€‚
             - *æ€ç»´æ¨¡å‹*ï¼šå“ˆå°”æ»¨ç«äº† -> æ€è€ƒ"æŠ—å¯’/å†·é“¾åŒ…è£…"ï¼›ç¹èŠ±çƒ­æ’­ -> æ€è€ƒ"å¤å¤/æ¸¯é£ç¤¼ç›’"ï¼›å¤šå·´èƒºç©¿æ­ -> æ€è€ƒ"é²œè‰³é…è‰²åŒ…è£…"ã€‚
           - **é«˜æ„å›¾è½¬åŒ–**ï¼šåŒ…å« [æœç´¢éœ€æ±‚]ã€[1688é‡‡è´­]ã€å¤šå°‘é’±ã€æ€ä¹ˆé€‰ã€‚
        2. **Açº§ï¼ˆé‡ç‚¹ - å•†ä¸šåœºæ™¯ï¼‰**ï¼š
           - åŒ…å« å°æ‰¹é‡ã€ç¤¼å“å®šåˆ¶ã€ä¼´æ‰‹ç¤¼ã€EtsyåŒ…è£…ã€ç§åŸŸåŒ…è£…ã€‚
           - å­£èŠ‚æ€§è¯é¢˜ï¼šæ˜¥èŠ‚ç¤¼ç›’ã€ç”µå•†å¤§ä¿ƒã€å±•ä¼šã€ç¯ä¿æ–°è§„ã€‚
        3. **Bçº§ï¼ˆç‰¹å®šå…³è”ï¼‰**ï¼š
           - æœ‰æ˜ç¡®å•†ä¸šä»·å€¼çš„è¡Œä¸šé•¿å°¾è¯ã€‚

        çƒ­æœåˆ—è¡¨ï¼š
        {trends_str}

        è¯·ä¸¥æ ¼è¿”å› JSON æ ¼å¼åˆ—è¡¨ï¼š
        [
            {{"topic": "è¯é¢˜å", "angle": "ç»“åˆè§’åº¦(å¦‚: å€ŸåŠ¿å“ˆå°”æ»¨çƒ­åº¦ï¼Œåˆ‡å…¥å†·é“¾åŒ…è£…åœºæ™¯)", "priority": "S"}}
        ]
        ä¸è¦è¿”å› Markdownã€‚
        """
        res = llm_utils.call_llm_json_array(prompt, temperature=0.7, max_retries=2)
        analyzed_trends = res if res else []
        
        # === Fallback: ç¡®ä¿æ•°é‡è¾¾æ ‡ ===
        trend_settings = input_data.get("config", {}).get("trend_settings", {})
        target_count = trend_settings.get("max_trends_to_analyze", 5)
        
        if len(analyzed_trends) < target_count:
            print(f"âš ï¸ [Topics] LLMä»…è¿”å› {len(analyzed_trends)} ä¸ª (ç›®æ ‡{target_count})ï¼Œå¯åŠ¨è‡ªåŠ¨è¡¥å…¨...")
            
            # 1. æå–å·²æœ‰çš„ topics ä»¥é¿å…é‡å¤
            existing_topics = {t.get("topic", "") for t in analyzed_trends}
            
            # 2. ä»åŸå§‹åˆ—è¡¨ä¸­å¯»æ‰¾å€™é€‰
            candidates = []
            for raw_t in trends:
                # ç®€å•æ¸…ç†ï¼šå»é™¤ "[å¹³å°]" å‰ç¼€
                clean_t = re.sub(r'\[.*?\]\s*', '', raw_t)
                if clean_t and clean_t not in existing_topics:
                    candidates.append(clean_t)
            
            # 3. éšæœºæŠ½å–è¡¥å…¨
            needed = target_count - len(analyzed_trends)
            if candidates:
                fillers = random.sample(candidates, min(needed, len(candidates)))
                for f in fillers:
                    analyzed_trends.append({
                        "topic": f,
                        "angle": "å…¨ç½‘çƒ­ç‚¹æµé‡æ‰¿æ¥",
                        "priority": "A"
                    })
            print(f"âœ… [Topics] å·²è¡¥å…¨è‡³ {len(analyzed_trends)} ä¸ª")
            
        return analyzed_trends[:target_count]

    def _generate_titles(self, trend, brand_config):
        brand_name = brand_config.get('brand', {}).get('name', 'ç›’è‰ºå®¶')
        topic = trend.get('topic', '')
        angle = trend.get('angle', '')
        
        # Get counts from config
        trend_settings = brand_config.get('trend_settings', {})
        count = trend_settings.get('titles_per_trend', 3)

        # åŠ¨æ€è·å–å½“å‰å¹´ä»½
        current_year = datetime.now().year

        prompt = f"""
        èƒŒæ™¯ï¼š{brand_name} (æ—¢æ¥B2Bå¤§å•ï¼Œä¹Ÿæ¥B2Cå°å•ï¼Œ**1ä¸ªèµ·è®¢**)
        çƒ­ç‚¹ï¼š{topic} (è§’åº¦: {angle})
        å½“å‰å¹´ä»½ï¼š{current_year}å¹´

        ä»»åŠ¡ï¼šç”Ÿæˆ {count} ä¸ªé«˜ç‚¹å‡»ç‡ Titleã€‚
        è¦æ±‚ï¼š
        1. **å¤šæ ·åŒ–å¥å¼ï¼ˆæ‹’ç»å…¬å¼åŒ– - ä¸¥å‰æ‰§è¡Œï¼‰**ï¼š
           - **ç»å¯¹è¿ç¦è¯** (å‡ºç°å³åˆ¤å®šä¸ºåŠ£è´¨)ï¼š
             - ä¸¥ç¦ä½¿ç”¨ "é«˜çº§æ„Ÿ(ç¤¼ç›’)çš„ç§˜å¯†"ã€"è¿˜åœ¨ä¸º...å‘æ„"ã€"å‘Šåˆ«(ä¹°å®¶ç§€)"
             - ä¸¥ç¦ä½¿ç”¨ "XXçš„æ­£ç¡®æ‰“å¼€æ–¹å¼"ã€"ä¸€æ–‡çœ‹æ‡‚"ã€"ä¸€ç«™å¼å¹³å°"
             - ä¸¥ç¦ä½¿ç”¨ "æ¡ˆä¾‹å¤ç›˜ï¼š"ã€"æ•…äº‹ï¼š"ã€"1ä¸ªèµ·è®¢ï¼Œ"ã€"2026å¹´"ï¼ˆé™¤éå¿…è¦ï¼‰
           - **æ‹’ç»æ’æ¯”**ï¼š{count}ä¸ªæ ‡é¢˜çš„å¥å¼å’Œå‰åŠå¥å¿…é¡»å®Œå…¨ä¸åŒã€‚
        2. **é«˜ç‚¹å‡»ç‡é£æ ¼ï¼ˆHuman-writtenï¼‰**ï¼š
           - **æ‚¬å¿µå‹**ï¼š"è¢œå­é”€é‡ç¿»å€ï¼Ÿæ²¡æƒ³åˆ°ä»…ä»…æ˜¯æ¢äº†è¿™ä¸ªåŒ…è£…..."
           - **ç›´å‡»ç—›ç‚¹**ï¼š"å°æ‰¹é‡å®šåˆ¶å¤ªè´µï¼Ÿæºå¤´å‚é•¿è¯´äº†çœŸè¯"
           - **æ•°æ®è¯´è¯**ï¼š"å®¢å•ä»·æå‡30%çš„ç§˜å¯†ï¼šæ­ç§˜å¤§ç‰Œç¤¼ç›’è®¾è®¡é€»è¾‘"
           - **é¿å‘æŒ‡å—**ï¼š"åŠé€€ï¼è¿™3ç§åŒ…è£…æè´¨åƒä¸‡åˆ«è¸©é›·"
        3. **å­—æ•°æ§åˆ¶**ï¼š30ä¸ªå­—ç¬¦ä»¥å†…ï¼ˆå…è®¸æ›´å®Œæ•´çš„é•¿æ ‡é¢˜ï¼‰ã€‚
        4. **å†…å®¹åˆ†å¸ƒ** ({count}ä¸ªæ€»æ•°)ï¼š
           - ç¡®ä¿è¦†ç›– **ä¸“ä¸šçŸ¥è¯†**ã€**è¡Œä¸šèµ„è®¯**ã€**äº§å“ä»‹ç»** ä¸­è‡³å°‘ä¸¤ä¸ªåˆ†ç±»ã€‚
           **æ³¨æ„**ï¼šä¸è¦è¿”å›ç‹¬ç«‹çš„"å®¢æˆ·æ¡ˆä¾‹"åˆ†ç±»ï¼Œå½’å…¥ä»¥ä¸Šä¸‰ç±»ã€‚

        è¿”å› JSON:
        [
            {{"title": "æ ‡é¢˜1", "category": "ä¸“ä¸šçŸ¥è¯†"}},
            ... (å…±{count}ä¸ª)
        ]
        """
        res = llm_utils.call_llm_json_array(prompt, temperature=0.7, max_retries=2)
        return res[:count] if res else []

    def _clean_category(self, cat):
        valid_cats = ["ä¸“ä¸šçŸ¥è¯†", "è¡Œä¸šèµ„è®¯", "äº§å“ä»‹ç»"]
        for v in valid_cats:
            if v in cat: return v
        return "è¡Œä¸šèµ„è®¯"
